\documentclass{article}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}

\newcommand\todo[1]{\textcolor{red}{TODO #1}}

\begin{document}

\title {Optimization Algorithms:\\
Mini-Batches, Exponentially Weighted Averages, and More}
\author{William Grim \\ \href{mailto:grimwm@gmail.com}{grimwm@gmail.com}}

\maketitle

\tableofcontents

\begin{abstract}
Training neural networks is a highly iterative process; so, it helps a lot to train a deep learning model quickly.  There are various optimization algorithms to help mitigate this empirical analysis cycle.
\end{abstract}

\section{Mini-Batch Gradient Descent}

In this section, we will be discussing matrices and vectors that look like the following:

\begin{gather}
\label{eqn:input_matrices}
X = \begin{bmatrix}
x^{(1)}, x^{(2)}, ..., x^{(1000)}, x^{(1001)}, ..., x^{(2000)}, ..., x^{(m)}
\end{bmatrix}\\
%
y = \begin{bmatrix}
y^{(1)}, y^{(2)}, ..., y^{(1000)}, y^{(1001)}, ..., y^{(2000)}, ..., y^{(m)}
\end{bmatrix}
\end{gather}

Remember that $X$ is an $(n_x, m)$ matrix and y is a $(1, m)$ vector.

\subsection{Gradient Descent: Vectorization and Mini-Batching}

Modern linear algebra libraries allow computer programs to computer matrix math very quickly.  For example, given an $(n_x, m)$ matrix and a $(1, m)$ vector as seen in \ref{eqn:input_matrices}, an NN can compute gradient descent quickly, assuming $m$ is a reasonable size.

However, what about when $m$ is very large?  Then, vectorization of gradient descent can still be too slow.

Assume for a moment that $m = 5,000,000$ and that this number of samples would be much too slow to handle directly.  Then, we could, for example, split $X$ into several mini-batches, $X^{\{1\}}, X^{\{2\}}, ..., X^{\{5000\}}$, each containing 1,000 entries from $X$.  Similarly, we will do the same for $y^{\{1\}}, y^{\{2\}}, ..., y^{\{5000\}}$.

When we iterate these, we use the new notation, $X^{\{t\}}, y^{\{t\}}$ to indicate the $t^{th}$ mini-batch.  If the mini-batch size is 1,000, as in our exmaple above, then the shapes of $X^{\{t\}}$ and $y^{\{t\}}$, respectively, are $(n_x, 1000), (1, 1000)$.

So, how is it implemented?  It's implemented exactly the same as normal gradient descent, also known as "batch gradient descent", except it has an additional loop that feeds the gradient descent algorithm piecemeal.  See Algorithm \ref{alg:mini_batch_grad}.

\begin{algorithm}[h]
\label{alg:mini_batch_grad}
\caption{Mini-Batch Gradient Descent}
\For{$i \gets 0$ \KwTo numiters}{
  \For{$t \gets 0$ \KwTo $\frac{m}{b}$}{
  parameters = grad\underline{ }descent($X^{\{t\}}, y^{\{t\}}$, parameters) \;
}}
\end{algorithm}

\subsubsection{Choosing Mini-Batch Size}

Assume $k = \frac{m}{b}$, where $m$ is the number of training samples,
$b$ is the number of items in the mini-batch.  Then, $s$ is the number of mini-batches.

\begin{enumerate}
\item If $s = m$, then we have \textbf{batch gradient descent}: $(X^{\{1\}}, Y^{\{1\}}) = (X, Y)$
  \begin{enumerate}
  \item Smooth gradient curve converging towards the global minimum,
        assuming all hyperparameters chosen correctly.
  \item Too long to compute per iteration.
  \end{enumerate}
\item If $s = 1$, then we have \textbf{stochastic gradient descent}: $(x^{\{t\}}, y^{\{t\}}) = x^{(t)}, y^{(t)})$
  \begin{enumerate}
  \item Bounces around the global minimum but does not completely converge; although, this can be ameliorated with slowing down the learning rate.
  \item Loses all the speedup from vectorization, because we are only processing one vector at a time rather than the full matrix.
  \end{enumerate}
\item If $1 < s < m$, it results in the fastest learning.
  \begin{enumerate}
  \item Allows vectorization.
  \item Makes incremental progress without waiting for an entire training set to complete.
  \item Not guaranteed to converge to the global minimum, but it does oscillate closely.  This can be ameliorated by slowing down the learning rate.
  \end{enumerate}
\end{enumerate}

Guidelines to choosing $s$:

\begin{enumerate}
\item If using small training set, let $s = m$
\item Typical mini-batch sizes: $64 \leq s \leq 512 | s = 2^k$
\item Make sure $s$ fits inside CPU / GPU memory
\end{enumerate}

\section{Exponentially Weighted Moving Average}

The formula for the \textbf{exponentially weighted moving average}\footnote{We will be calling this the \textbf{exponentially weighted average} as well} is given by

\begin{equation}
V_t = \beta V_{t-1} + (1-\beta) \theta_t
V_0 = 0
\end{equation}

Where $\theta_t$ is the unit being measured and $\beta$ is the weight being applied to the previous value, $V_{t-1}$.

Given $\beta$, then $V_t \approx$ last $\frac{1}{1-\beta}$'s unit measurement (e.g. if using day's temperature and $\beta = 0.9$, then $V_t \approx $ average of last $10$ days' temperature.

The reason for why this is called the \textbf{exponentially weighted moving average} can be shown fairly straightforwardly:

\begin{gather}
V_t = (1 - \beta) \theta_t + \beta V_{t-1} \\
V_t = (1 - \beta) \theta_t + \beta ((1-\beta) \theta_{t-1} + \beta V_{t-2}) \\
V_t = (1 - \beta) \theta_t + \beta (1-\beta) \theta_{t-1} + \beta^2 (1-\beta) \theta_{t-2} + ... + \beta^t (1-\beta) \theta_0 \label{eqn:ewma}
\end{gather}

Summing just the coefficients from \ref{eqn:ewma}, we have

\begin{align}
(1-\beta) + \beta(1-\beta) + \beta^2(1-\beta) + ... + \beta^t(1-\beta) \\
= \lim_{t \rightarrow \infty} \left( (1-\beta) \sum_{i=0}^{t} \beta^i \right) \approx 1
\end{align}

\subsection{Time Constant}

The value for how many days are averaged is decided by an \href{https://en.wikipedia.org/wiki/Exponential_smoothing#Time_Constant}{"arbitrary" time constant} chosen to be

\begin{equation}
\tau = (1-\epsilon)^{\frac{1}{\epsilon}} \approx \frac{1}{e} \label{eqn:tau}
\end{equation}

Equation \ref{eqn:tau} is chosen so that our signal smooths to approximately $\frac{1}{e}$ after just $(1-\epsilon)^\frac{1}{\epsilon}$ previous data points.

\subsection{Concrete Example}

Given that our time scale is in days and

\begin{align}
\beta = 0.9 \\
1-\beta = \epsilon = 0.1
\end{align}

Then,

\begin{align}
\tau = (1 - \epsilon)^\frac{1}{\epsilon} \\
= \beta^\frac{1}{1-\beta} \\
= 0.9^{10} \approx \frac{1}{e}
\end{align}

Or, in other words, $V_t = 0.9 V_{t-1} + 0.1 \theta_t$ is averaging about the last 10 days.

\subsection{EWMA vs Simple Average}

The EWMA is computationally superior to the simple average, even though it is not quite as accurate.  For example, the simple average will be a correct average for the last 10 days, while the EWMA will be some approximation to that average over the same time window.  However, the EWMA can be calculated using $O(1)$ space and $O(n)$ steps, making it superior in terms of space complexity.

\subsection{Bias Correction in Exponentially Weighted Moving Averages}

For the initial values of $V_0, V_1, V_2, ...$, $V_t$ will not properly represent the initial values.  So, to correct for the multiplying $\theta_1, \theta_2, \theta_3, ...$ by $(1-\beta)$, we will boost the values like this:

\begin{align}
\frac{V_t}{1-\beta^t}
\end{align}

Having said all this, for most implementations of machine learning algorithms, most people don't bother with bias correction, because they wait for enough data to remove the bias.  However, if someone is worried about bias during the "warmup period", then bias correction is useful.

\section{Gradient Descent with Momentum}

Summary: compute the \textit{exponentially weighted average} of the gradients during backpropagation and then use these gradients to update the weights.

Remember that \textbf{mini-batch gradient descent} causes oscillations towards the global minimum, which slows down gradient descent and prevents the use of a larger learning rate.  In fact, a larger learning rate could cause diversion from the global minimum (exploding gradients).

\begin{algorithm}[h]
\label{alg:grad_with_momentum}
\caption{Gradient Descent with Momentum}
Compute dW,db on current mini-batch. \;
$V_{dW} = \beta V_{dW} + (1-\beta) dW$ \;
$V_{db} = \beta V_{db} + (1-\beta) db$ \;
$W = W - \alpha V_{dW}$ \;
$b = b - \alpha V_{db}$ \;
\end{algorithm}

\paragraph{Why is it called momentum?}

Imagine a ball rolling down hill to the global minimum.  The derivatives act as \textit{acceleration}, and the stored average acts as the \textit{velocity} while $\beta$ acts as \textit{friction} to slow the ball down.  Therefore, over time, the ball can gain \textit{momentum} towards the global minimum without veering off course (e.g. \textit{exploding gradients}).

\paragraph{Choice of $\beta$}

In practice, $\beta = 0.9$ is a very common choice.  Also, \textit{bias correction} is not normally done with this value of $\beta$, because after only 10 iterations, the moving average will have "warmed up" and no longer be biased.

\paragraph{Implementation Notes}

$V_{dW} = 0$ is actually a matrix of zeros with the same dimensions as $dW$ and $W$.  Likewise, $V_{db} = 0$ is actually a vector of zeros with the same dimensions as $db$ and $b$.

\paragraph{Notes on "The Literature"}

Commonly, EWMAs are iterated without multiplying the current derivatives by $(1-\beta)$.  The net effect is that the EWMAs are scaled by a factor of $(1-\beta)$, and so the learning rate must be scaled by $\frac{1}{1-\beta}$, which is not all that intuitive and could lead to having to recalibrate $\alpha$ when calibrating $\beta$.

\section{RMSProp: Root Mean Squared Propagation}

\begin{algorithm}[h]
\label{alg:rmsprop}
\caption{Root Mean Squared Propagation}
Compute dW,db on current mini-batch \;
$S_{dW} = \beta S_{dW} + (1-\beta) dW^2$ \; \label{line:rmsprop_sdw}
$S_{db} = \beta S_{db} + (1-\beta) db^2$ \; \label{line:rmsprop_sdb}
$W = W - \alpha \frac{dW}{\sqrt{S_{dW}} + \epsilon}$ \;
$b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}$ \;
\end{algorithm}

It should be noted that Lines \ref{line:rmsprop_sdw} and \ref{line:rmsprop_sdb} from Algorithm \ref{alg:rmsprop} are performing element-wise squaring.  So, in essence, these are EWMAs of the squared weights and intercepts.

Due to the nature of our derivatives, $db$ tends to be very large and $dW$ tends to be very small.  So, if $b$ is the "y-axis" and $W$ is the "x-axis", then doing the divisions by $\sqrt{x}$ will tamp down $b$ oscillations a lot while increasing the oscillation of $W$, which is our desired goal.

\paragraph{Important note on $\beta$}

We will call the $\beta$ used in \textit{RMSProp} as $\beta_2$ going forward, as we combine both \textit{RMSProp} and \textit{Gradient Descent with Momentum}.

\paragraph{What is $\epsilon$?}

This exists to prevent "divide by 0" errors by adding a very small number to the denominator.

\paragraph{RMSProp Fun Fact}

\textit{RMSProp} wasn't introduced in an academic paper.  It was introduced in an online \textit{Coursera} course.

\section{Adam Optimization Algorithm}

\textit{Adam Optimization} stands for "adaptive moment estimation optimization" and is going to be a combination of both \textit{Gradient Descent with Momentum} and \textit{RMSProp}, as seen in Algorithm \ref{alg:adam_opt}.

\begin{algorithm}[h]
\label{alg:adam_opt}
\caption{Adam Optimization Algorithm}
$V_{dW} = 0$ \;
$S_{dW} = 0$ \;
$V_{db} = 0$ \;
$S_{db} = 0$ \;

\For{$i \gets 0$ \KwTo numiters}{
  \For{$t \gets 0$ \KwTo $\frac{m}{b}$}{
    \tcc{Compute gradient descent using mini-batch}
    parameters = grad\underline{ }descent($X^{\{t\}}$, $y^{\{t\}}$, parameters) \;
    dW = parameters["dW"] \;
    db = parameters["db"] \;
    
    \tcc{Compute momentum}
    $V_{dW} = \beta_1 V_{dW} + (1-\beta_1) dW$ \;
    $V_{db} = \beta_1 V_{db} + (1-\beta_1) db$ \;
    
    \tcc{Compute RMSProp}
    $S_{dW} = \beta_2 S_{dW} + (1-\beta_2) dW^2$ \;
    $S_{db} = \beta_2 S_{db} + (1-\beta_2) db^2$ \;
    
    \tcc{Compute bias correction}
    $V_{dW}^{b} = \frac{V_{dW}}{1-\beta_1^t}$ \;
    $V_{db}^{b} = \frac{V_{db}}{1-\beta_1^t}$ \;
    $S_{dW}^{b} = \frac{V_{dW}}{1-\beta_1^t}$ \;
    $S_{db}^{b} = \frac{V_{db}}{1-\beta_1^t}$ \;
    
    \tcc{Update}
    $W = W - \alpha\frac{V_{dW}^{b}}{\sqrt{S_{dW}^{b}} + \epsilon}$ \;
    $b = b - \alpha\frac{V_{db}^{b}}{\sqrt{S_{db}^{b}} + \epsilon}$ \;
  }
}
\end{algorithm}

Please see the paper, \href{https://arxiv.org/pdf/1412.6980.pdf}{"Adam: A Method for Stochastic Optimization"} for more details.

\paragraph{Hyperparameter Choices}

\begin{itemize}
\item $\alpha$ needs to be tuned
\item $\beta_1$ is typically 0.9 and is related to the \textit{expoentially weighted moving average}
\item $\beta_2$ is recommended to be 0.999 and is related to the squared values of the EWMA
\item $\epsilon$ doesn't matter a lot but is recommended to be $10^{-8}$
\end{itemize}

\section{Learning Rate Decay}

The intuition is that at first, there will be some noise, but as the algorithm gets closer to the global minimum, we should slow down our learning rate to get better approximations.

\paragraph{Terminology}

One \textbf{epoch} will be one pass through an entire batch of data.  Thus, in the case of mini-batches, it will be one pass through all the mini-batches.

Basically, the \textit{learning rate decay} is defined as follows:

\begin{align}
\alpha = \frac{\alpha_0}{1 + d * epoch}
\end{align}

Where $d$ is our decay rate.

\subsection{Example}

Let $\alpha_0 = 0.2$ and $d = 1$:

\begin{tabular}{c | l}
epoch & $\alpha$ \\ \hline
1 & 0.100 \\
2 & 0.067 \\
3 & 0.050 \\
4 & 0.040 \\
... & ...
\end{tabular}

\subsection{Other Decay Methods}

\paragraph{Exponential Decay}

$\alpha = n^{epoch} \alpha_0$, where $0 < n < 1$

\paragraph{Root Decay}

$\alpha = \frac{k}{\sqrt{epoch}} \alpha_0$ or $\alpha = \frac{k}{\sqrt{t}} \alpha_0$

\paragraph{Discrete Decay}

Every so many iterations of $t$, decay $\alpha$ by a fixed amount.

\paragraph{Manual Decay}

Sometimes, when working with just a few number of models that train over the course of several hours or several days, someone might manually change $\alpha$ based on noticing that the model learning rate has slowed down.

\section{The Problem of Local Optima}

In the early days of neural networks, plotting a graph in 2-d or 3-d space would make it appear as though it would be easy to hit a local optima and optimize that instead of the global minimum.  However, this initial intuition was wrong.  With high-dimensional data (e.g. $d \geq 20000$), the chances of that happening are very low, and what normally happens is that a \textit{saddle point} is hit, because the chances that all, e.g. 20,000, dimensions are facing the same direction is quite low.

\paragraph{Plateaus}

Instead, \textit{plateaus} can be more a problem and slow down learning.  \textit{Plateaus} basically can have slopes close to 0 for a very long time.  This is an area where \textit{momentum} and \textit{RMSProp} can help get off the plateaus.

\end{document}