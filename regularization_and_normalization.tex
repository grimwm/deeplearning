\documentclass{article}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsmath}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}

\begin{document}

\title {Regularization of NN}
\author{William Grim \\ \href{mailto:grimwm@gmail.com}{grimwm@gmail.com}}

\maketitle

\begin{abstract}
When you build NN, you will find that in many cases, you end up either with underfitting, known as bias, or overfitting, known as variance.  Regularization aids you in removing variance at the expense of potentially adding bias if you add too much regularization to the cost function, $J$.
\end{abstract}

\section{Regularization}

The most often used regularization is what is known as $L_{2}$ regularization, but the more formal name for it is the scaled square of the "Frobenius Normal" or analogously, the "Euclidean Normal".  Although, in practice, the "Euclidean Normal" is a bit of a misnomer when dealing with matrices, but you will quickly see the similarities between the "Frobenius Normal" and Euclidean distance normalization for a vector.

\subsection{L2 Regularization}

\subsection{Frobenius or Euclidean Norm}

In general, $L_2$ regularization is based on the concept of the Frobenius norm, also known as the Euclidean norm.  It is essentially a measure of the distance amongst all the dimensions in a matrix, $M$, or vector, $v$.

\begin{align} \label{eqn:frobenius}
||M||_{F} = \sqrt(\sum_{i=1}^{R} \sum_{j=1}^{C} W_{ij}^{2})
\end{align}

Without further adieu, refer to Equation \ref{eqn:frobenius_W}.

\begin{align} \label{eqn:frobenius_W}
||W||_{F} = \sqrt(\sum_{i=1}^{n^{l}} \sum_{j=1}^{n^{l-1}} W_{ij}^{2})
\end{align}

Where
\begin{itemize}
\item $n^{l}$ = number of features in layer l
\item $n^{l-1}$ = number of features in layer l-1.
\end{itemize}

Finally, the $L_{2}$ regularization is just

\begin{gather}
L_{2} = \frac{\lambda}{2m} \sum_{l=1}^{L} ||W||_{F}^{2}
\end{gather}

Where $\frac{\lambda}{2m}$ is just a scaling constant and can be virtually anything, but having $2$ in the denominator makes the derivative a bit nicer.  Speaking of which:

\begin{gather}
\frac{d[L_{2}]}{dW} = \frac{\lambda}{m} \sum_{l=1}^{L} W
\end{gather}

\subsection{L1 Regularization}

$L_{1}$ is a less-used regularization, with the idea being that $W$ could be made more sparse (e.g. with more 0s).  You will have to experiment with whether or not this style of regularization would work for you.  Its formula is seen in Equation \ref{eqn:l1}

\begin{gather} \label{eqn:l1}
L_{1} = \frac{\lambda}{2m} ||W||_{F} \\
\frac{d[L_{1}]}{dW} = \frac{\lambda}{2m}
\end{gather}

\subsection{Cost Function, J}

Using the newly learned $L_2$ regularization, let's see how it would apply to the cost function, $J$.

\begin{gather}
J(W, b) = \frac{1}{m} \sum(\hat{y}, y) + \frac{\lambda}{2m} ||W||_{F}^{2}
\end{gather}

\subsection{Back Propagation}

Again, using $L_2$ regularization, let's see how it applies to back propagation values $dW$ and $db$.

\begin{gather}
dW^{[l]} = D + \frac{\lambda}{m} W^{[l]} \\
W = W - \alpha dW = W - \alpha (D + \frac{\alpha \lambda}{m} W^{[l]} \\
= W(1 - \frac{\lambda}{m}) - \alpha D
\end{gather}

Where $D$ is the value computed from normal back propagation without regularization.  Hence, regularization is added to the derivative AFTER the derivative is calculated in back propagation.  It almost would seem better just to call it $dW$, but I want to stress the point that $D$ is calculated first and separately.

Due to the term $W - \frac{\alpha\lambda}{m} W$, $L_2$ regularization is also known as \textbf{weight decay}, because we end up multiplying $W$ by $(1 - \frac{\alpha\lambda}{m})$, which ends up multiplying $W$ by a value just a bit under $1$.  Therefore, $L_2$ is decaying $W$.

\section{Normalization}

We often want to normaliza so that our features become more distance, allowing the cost function, $J$, to look more symmetric, allowing us to use a more lenient learning rate, $\alpha$.  With an unnormalized $J$, we often have a "squished" function where $\alpha$ must be chosen correctly and often results in slower learning.

Subtract the mean:

\begin{gather}
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \\
x := x - \mu
\end{gather}


Normalize variance:

\begin{gather}
\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)})^2 \\
x = x \div \sigma^2
\end{gather}

NOTE: Use the same $\mu$ and $\sigma^2$ on your test set as you did on your training set so that you can compare apples to apples.

\section{Vanishing and Exploding Gradients}

A \textbf{vanishing gradient} is one that a gradient becomes so shallow that it becomes difficult to find the optimal cost because we move along the curve $J$ very slowly.  An \textbf{exploding gradient} is one that becomes so steep that it becomes difficult to find the optimal cost because we move along the curve $J$ too quickly.

Assume a very deep L-layer network with just two nodes per hidden unit.  For the sake of argument, also assume we are using the linear activation function, $g(W, A) = W * A$.  We end up in this situation for $\hat{y}$ as seen in Equation \ref{eqn:weight_exponentiation}:

\begin{gather}
\hat{y} = g(W^{[L]}, g(W^{[L-1]}, ..., g(W^{[2]}, g(W^{[1]}, X))...)) \\
\hat{y} = W^{[L]} W^{[l-1]} ... W^{[3]} W^{[2]} W^{[1]} X \label{eqn:weight_exponentiation}
\end{gather}

On its own, this is not a problem, but imagine two scenarios where all of $W^{[1]}, ..., W^{[L]}$ are one of the following $W_1$ or $W_2$:

\begin{gather}
W_1 = \begin{bmatrix}
1.5 & 0.0 \\
0.0 & 1.5
\end{bmatrix} \\
%
W_2 = \begin{bmatrix}
0.5 & 0.0 \\
0.0 & 0.5
\end{bmatrix}
\end{gather}

\paragraph{Scenario 1}

As we multiply $W_1$ by itself multiple times, we will find that we end up with the $1.5$ values becoming $1.5 * 1.5 * ... = 1.5^L$, which results in an exploding gradient.

\paragraph{Scenario 2}

As we multiply $W_2$ by itself multiple times, we will find that we end up with the $0.5$ values becoming $0.5 * 0.5 * ... = 0.5^L$, which results in a vanishing gradient.

\subsection{Proper Weight ($W$) Initialization}

Assume 1-layer neural network with 4 input features, $x$.  Then, $z = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b$.  Therefore, to prevent $z$ from exploding, as $n -> \infty$, we want $w_i -> 0$.

$W^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]}})$

\section{Gradient Checking}

It is important to test your deep learning model's gradients against the actual gradients to see that you have achieved an accurate approximation.

\subsection{Numerical Approximation of Gradients: Two-Sided Rule}

First, let's describe a numerical method of approximating gradients.  Assume that $f(x)$ is continuous and smooth curve on the domain $[a,b]$  Then, the exact derivative is $f'(x)$ on $[a,b]$.  Also, it is well known that the derivative of $f'(x)$ can be written as

\begin{align}
f'(x) = \lim_{\epsilon \to 0} \frac{f(x + \epsilon) - f(x - \epsilon)}{(x + \epsilon) - (x - \epsilon)} \\
= \lim_{\epsilon \to 0} \frac{f(x + \epsilon) - f(x - \epsilon)}{2 \epsilon}
\end{align}

So, given $f(x)$ and $f'(x)$, we can compute a numerical approximation of $f'(x)$ using this formula:

\begin{align}
g(x) = \frac{f(x + \epsilon) - f(x - \epsilon)}{(x + \epsilon) - (x - \epsilon)} \\
= \frac{f(x + \epsilon) - f(x - \epsilon)}{2 \epsilon}
\end{align}

Where $\epsilon$ is some small, fixed value, such as 0.01.

\subsubsection{Example}

Assume $f(x) = x^3$ and $\epsilon = 0.01$.  Then,

\begin{gather}
f'(x) = 3 x^2 \\
g(x) = \frac{f(x + 0.01) + f(x - 0.01)}{2 \epsilon}
\end{gather}

Now assume we choose $x = 3$ as a test location:

\begin{gather}
f'(3) = 27 \\
g(3) = \frac{27.2709 - 26.7309}{0.02} = 27.0001
\end{gather}
\\
Making $g(x)$ a good approximation to $f'(x)$ within 0.0001 units.

\subsection{Gradient Checking in Practice}

Reshape $W^{[1]}$, $b^{[1]}$, ... $W^{[L]}$, $b^{[L]}$ and $dW^{[1]}$, $db^{[1]}$, ... $dW^{[L]}$, $db^{[L]}$ into large, consolidated vectors, $\theta$ and $d\theta$, respectively.

Now, our cost function is $J(\theta)$ instead of $J(W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]})$.  Therefore, we can write $J(\theta)$ as

\begin{align}
J(\theta) = J(\theta_1, \theta_2, ..., \theta_L)
\end{align}

For each dimension (or layer) of the function, we must compute $d\theta_{approx}$ as in Algorithm \ref{alg:grad_check}:

\begin{algorithm}[h]
\caption{Gradient Check}
\label{alg:grad_check}
\For{i:=1 to L}{
$d\theta_{approx}[i] := \frac{J(\theta_1, \theta_2, ..., \theta_i + \epsilon, ..., \theta_L) + J(\theta_1, \theta_2, ..., \theta_i - \epsilon, ..., \theta_L)}{2 \epsilon}$\;
}
\end{algorithm}

If $\theta$ has been chosen wisely, then

\begin{align}
d\theta_{approx}[i] \approx \frac{\partial{J}}{\partial{\theta_i}}
\end{align}

To compute the above formula, you can use the normalized Frobenius distance (e.g. Euclidean distance) referenced from Equation \ref{eqn:frobenius}:

\begin{align}
e = \frac{||d\theta_{approx} - d\theta||_2}{||d\theta_{approx}||_2 + ||d\theta||_2||}
\end{align}

In general, if you choose any value $\epsilon$ and the ratio above yields $e \leq \epsilon$, then your approximation is most likely good.  If your approximation is off by two orders of magnitude, it may be alright, but you will need to check all your components.  If it's off by higher orders of magnitude, it is most likely incorrect.

\subsubsection{Implementation Notes}

\begin{itemize}
\item Don't use gradient checking in training, only to debug, because it is very slow.
\item If grad check fails, look at each component of $d\theta_{approx}$ to identify which layer is causing problems.  You should notice that the component is quite a bit different than the similar component in $d\theta$.
\item Grad check doesn't work with dropout regularization, because dropout is randomized and makes it difficult to compute $J$.
\item Perform random initialization and train the model for a while before running grad check to let $W$ and $b$ wander away from the $\approx 0$ starting values.
\end{itemize}

\end{document}