\documentclass{article}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}

\newcommand\todo[1]{\textcolor{red}{TODO #1}}

\begin{document}

\title {Bias and Variance: Regularization and Normalization}
\author{William Grim \\ \href{mailto:grimwm@gmail.com}{grimwm@gmail.com}}

\maketitle

\begin{abstract}
When you build NN, you will find that in many cases, you end up either with underfitting, known as bias, or overfitting, known as variance.  Regularization aids you in removing variance at the expense of potentially adding bias if you add too much regularization to the cost function, $J$.
\end{abstract}

\section{Training, Development, and Test Sets}

Historically, given data, we might split data sets of 100, 1,000, or 10,000 data points:

\begin{itemize}
\item 70\% training; 30\% test
\item 60\% training; 20\% dev / cross-validation; 20\% test
\end{itemize}

However, in the "Big Data" era, where you could end up with millions of data points, let's look at an example.  If you have 1,000,000 total data points, you might find you only need 10,000 cross-validation points and 10,000 test points, giving you a ratio of train/dev/test to be 98\%/1\%/1\%.

THe main point being that you can have very large training set ratios when compared to your dev and test sets.

\subsection{Data Point Distribution}

To split the data points out to training, dev, and test sets, then they should all come from the same data distribution.  In other words, if the model is learning only clearly defined cat photos but users will be using the model to look at blurry photos, then the training data must include blurry photos as well.

\subsection{No Dev Set}

Oftentimes, it is okay not to have a dev set, using the test set as the dev set.  In this setup, the nomenclature for the "dev set" is often "test set", even though it may not be very precise naming.

\section{Bias and Variance}

In a nutshell, \textbf{bias} is when a model is underfitting data, and \textbf{variance} is when a model is overfitting the data.  Basically, if the model is underfitting the data, the model will "bias" itself to more often than not wanting to say something is X when it is actually Y.  Variance, on the other hand, is when a model, more often than not, will fit the data so well that it can appear like the model is changing its classifications quite a lot, even with subtle changes in input.

\todo{Add 2-feature graphics}

\subsection{Example: Cats and Dogs}

Generally speaking, humans are very adept at knowing the difference between a cat and dog on sight.  So, what if a learning model was trained to do the same thing?

\paragraph{Variance}

Assume,

\begin{gather}
Train set error: 1\% \\
Dev set error: 11\%
\end{gather}

The model has obviously learned how to classify the training set properly, but it is doing poorly with the dev set.  In this situation, the model is overfitting the training set and "varying" a lot with the dev set.  For example, the model might be saying that cats are dogs and dogs are cats way more often than it should be, and it isn't clear which direction the model will go on any given input.

In each of the following examples of classifying cats vs. dogs, we assume that humans have $\approx 0\%$ error performing this task.  Or, in math nomenclature, we say that the \textbf{Optimal Bayes Error} is $\approx 0\%$.

\paragraph{Bias}

Assume,

\begin{gather}
Train set error: 15\% \\
Dev set error: 16\%
\end{gather}

In this case, the model is not doing well on either the training set of the dev set.  At the very least, the model is not varying much from the training set, but it appears to be underfitting or "biased" towards certain inputs.  For example, it is possible that it is often misclassifying cats as dogs but not necessarily dogs as cats.

\paragraph{Bias and Variance}

Assume,

\begin{gather}
Train set error: 15\% \\
Dev set error: 30\%
\end{gather}

Return from whence ye came; this is really bad: high bias and high variance.

\paragraph{Correct}

Assume,

\begin{gather}
Train set error: 0.5\% \\
Dev set error: 1\%
\end{gather}

This model is doing quite well with both low bias and low variance based on how well humans would do at this task, or how well it aligns with our optimal Bayes error.

\subsection{Counter-Tactics}

Here are some strategies to combat bias vs. variance.  Ask these questions and solve them in this order:

\begin{itemize}
\item High bias?
  \begin{itemize}
  \item Make a bigger NN with more hidden units or layers.
  \item Train longer
  \item Try optimization algorithms
  \item Try different NN architecture
  \end{itemize}
\item High variance?
  \begin{itemize}
  \item More data
  \item Regularization
  \item Try different NN architecture
  \end{itemize}
\end{itemize}

\subsection{Bias-Variance Tradeoff}

It used to be a problem that the tools available to reduce bias or variance would negatively impact the other.  However, these days, it is not as much of a problem.

\section{Regularization}

The most often used regularization is what is known as $L_{2}$ regularization, but the more formal name for it is the scaled square of the "Frobenius Normal" or analogously, the "Euclidean Normal".  Although, in practice, the "Euclidean Normal" is a bit of a misnomer when dealing with matrices, but you will quickly see the similarities between the "Frobenius Normal" and Euclidean distance normalization for a vector.

\subsection{Frobenius or Euclidean Norm}

In general, $L_2$ regularization is based on the concept of the Frobenius norm, also known as the Euclidean norm.  It is essentially a measure of the distance amongst all the dimensions in a matrix, $M$, or vector, $v$.

\begin{align} \label{eqn:frobenius}
||M||_{F} = \sqrt(\sum_{i=1}^{R} \sum_{j=1}^{C} W_{ij}^{2})
\end{align}

\subsection{L2 Regularization}

Without further adieu, refer to Equation \ref{eqn:frobenius_W}.

\begin{align} \label{eqn:frobenius_W}
||W||_{F} = \sqrt(\sum_{i=1}^{n^{l}} \sum_{j=1}^{n^{l-1}} W_{ij}^{2})
\end{align}

Where
\begin{itemize}
\item $n^{l}$ = number of features in layer l
\item $n^{l-1}$ = number of features in layer l-1.
\end{itemize}

Finally, the $L_{2}$ regularization is just

\begin{gather}
L_{2} = \frac{\lambda}{2m} \sum_{l=1}^{L} ||W||_{F}^{2}
\end{gather}

Where $\frac{\lambda}{2m}$ is just a scaling constant and can be virtually anything, but having $2$ in the denominator makes the derivative a bit nicer.  Speaking of which:

\begin{gather}
\frac{d[L_{2}]}{dW} = \frac{\lambda}{m} \sum_{l=1}^{L} W
\end{gather}

\subsubsection{Cost Function, J}

Using the newly learned $L_2$ regularization, let's see how it would apply to the cost function, $J$.

\begin{gather}
J(W, b) = \frac{1}{m} \sum(\hat{y}, y) + \frac{\lambda}{2m} ||W||_{F}^{2}
\end{gather}

\subsubsection{Back Propagation}

Again, using $L_2$ regularization, let's see how it applies to back propagation values $dW$ and $db$.

\begin{gather}
dW^{[l]} = D + \frac{\lambda}{m} W^{[l]} \\
W = W - \alpha dW = W - \alpha (D + \frac{\alpha \lambda}{m} W^{[l]} \\
= W(1 - \frac{\lambda}{m}) - \alpha D
\end{gather}

Where $D$ is the value computed from normal back propagation without regularization.  Hence, regularization is added to the derivative AFTER the derivative is calculated in back propagation.  It almost would seem better just to call it $dW$, but I want to stress the point that $D$ is calculated first and separately.

Due to the term $W - \frac{\alpha\lambda}{m} W$, $L_2$ regularization is also known as \textbf{weight decay}, because we end up multiplying $W$ by $(1 - \frac{\alpha\lambda}{m})$, which ends up multiplying $W$ by a value just a bit under $1$.  Therefore, $L_2$ is decaying $W$.

\subsection{L1 Regularization}

$L_{1}$ is a less-used regularization, with the idea being that $W$ could be made more sparse (e.g. with more 0s).  You will have to experiment with whether or not this style of regularization would work for you.  Its formula is seen in Equation \ref{eqn:l1}

\begin{gather} \label{eqn:l1}
L_{1} = \frac{\lambda}{2m} ||W||_{F} \\
\frac{d[L_{1}]}{dW} = \frac{\lambda}{2m}
\end{gather}

\subsection{Dropout Regularization}

Basically, at each layer of the network, layers are randomly removed if $T > P$ where "T" is a random number toss and "P" is the probability that the node should remain.  Essentially, the larger "P" is (up to 1.0), the higher the chance the node will \textit{not} be removed.

\textbf{Note}: Dropout regularization should only be used during the training phase.  Otherwise, the model will give non-deterministic results at test and production times.  Besides, regularization is only used to help models during their training period, which is clearer to understand by reading \href{https://en.wikipedia.org/wiki/Regularization_(mathematics)}{Wikipedia's article on regularization}.

\subsubsection{One Implementation: Inverted Dropout}

\begin{algorithm}[h]
\caption{Inverted Dropout}
$d_3 \gets np.random.rand(a_3.shape[0], a_3.shape[1]) < P$ \;
$a_3 \gets a_3 * d_3$ \;
$a_3 \gets a_3 / P$ \; \label{line:inverted_dropout}
\end{algorithm}

Line \ref{line:inverted_dropout} in our algorithm above is scaling $a_3$ up by the number of nodes that were removed.  Otherwise, when $z^{[3]} = W^{[3]} a^{[3]} + b^{[3]}$, $a^{[3]}$ is calculated, it will have been reduced by a proportional amount, $P$.  So, dividing $a_3$ by $P$ is called \textit{inverted dropout} and prevents $z^{[3]}$ from being reduced.

The real impetus for "inverted dropout" is not necessarily for the training set.  However, it will ensure that all our activations are in the same scale as our test set, making testing easier.

\subsubsection{Test Time}

Do not use dropout regularization (e.g. set $P = 0$), because it will make our model random, and that is difficult to test.

\section{Normalization}

We often want to normaliza so that our features become more distance, allowing the cost function, $J$, to look more symmetric, allowing us to use a more lenient learning rate, $\alpha$.  With an unnormalized $J$, we often have a "squished" function where $\alpha$ must be chosen correctly and often results in slower learning.

Subtract the mean:

\begin{gather}
\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \\
x := x - \mu
\end{gather}


Normalize variance:

\begin{gather}
\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)})^2 \\
x = x \div \sigma^2
\end{gather}

NOTE: Use the same $\mu$ and $\sigma^2$ on your test set as you did on your training set so that you can compare apples to apples.

\section{Vanishing and Exploding Gradients}

A \textbf{vanishing gradient} is one that a gradient becomes so shallow that it becomes difficult to find the optimal cost because we move along the curve $J$ very slowly.  An \textbf{exploding gradient} is one that becomes so steep that it becomes difficult to find the optimal cost because we move along the curve $J$ too quickly.

Assume a very deep L-layer network with just two nodes per hidden unit.  For the sake of argument, also assume we are using the linear activation function, $g(W, A) = W * A$.  We end up in this situation for $\hat{y}$ as seen in Equation \ref{eqn:weight_exponentiation}:

\begin{gather}
\hat{y} = g(W^{[L]}, g(W^{[L-1]}, ..., g(W^{[2]}, g(W^{[1]}, X))...)) \\
\hat{y} = W^{[L]} W^{[l-1]} ... W^{[3]} W^{[2]} W^{[1]} X \label{eqn:weight_exponentiation}
\end{gather}

On its own, this is not a problem, but imagine two scenarios where all of $W^{[1]}, ..., W^{[L]}$ are one of the following $W_1$ or $W_2$:

\begin{gather}
W_1 = \begin{bmatrix}
1.5 & 0.0 \\
0.0 & 1.5
\end{bmatrix} \\
%
W_2 = \begin{bmatrix}
0.5 & 0.0 \\
0.0 & 0.5
\end{bmatrix}
\end{gather}

\paragraph{Scenario 1}

As we multiply $W_1$ by itself multiple times, we will find that we end up with the $1.5$ values becoming $1.5 * 1.5 * ... = 1.5^L$, which results in an exploding gradient.

\paragraph{Scenario 2}

As we multiply $W_2$ by itself multiple times, we will find that we end up with the $0.5$ values becoming $0.5 * 0.5 * ... = 0.5^L$, which results in a vanishing gradient.

\subsection{Proper Weight ($W$) Initialization}

Assume 1-layer neural network with 4 input features, $x$.  Then, $z = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b$.  Therefore, to prevent $z$ from exploding, as $n -> \infty$, we want $w_i -> 0$.

$W^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]}})$

\section{Gradient Checking}

It is important to test your deep learning model's gradients against the actual gradients to see that you have achieved an accurate approximation.

\subsection{Numerical Approximation of Gradients: Two-Sided Rule}

First, let's describe a numerical method of approximating gradients.  Assume that $f(x)$ is continuous and smooth curve on the domain $[a,b]$  Then, the exact derivative is $f'(x)$ on $[a,b]$.  Also, it is well known that the derivative of $f'(x)$ can be written as

\begin{align}
f'(x) = \lim_{\epsilon \to 0} \frac{f(x + \epsilon) - f(x - \epsilon)}{(x + \epsilon) - (x - \epsilon)} \\
= \lim_{\epsilon \to 0} \frac{f(x + \epsilon) - f(x - \epsilon)}{2 \epsilon}
\end{align}

So, given $f(x)$ and $f'(x)$, we can compute a numerical approximation of $f'(x)$ using this formula:

\begin{align}
g(x) = \frac{f(x + \epsilon) - f(x - \epsilon)}{(x + \epsilon) - (x - \epsilon)} \\
= \frac{f(x + \epsilon) - f(x - \epsilon)}{2 \epsilon}
\end{align}

Where $\epsilon$ is some small, fixed value, such as 0.01.

\subsubsection{Example}

Assume $f(x) = x^3$ and $\epsilon = 0.01$.  Then,

\begin{gather}
f'(x) = 3 x^2 \\
g(x) = \frac{f(x + 0.01) + f(x - 0.01)}{2 \epsilon}
\end{gather}

Now assume we choose $x = 3$ as a test location:

\begin{gather}
f'(3) = 27 \\
g(3) = \frac{27.2709 - 26.7309}{0.02} = 27.0001
\end{gather}
\\
Making $g(x)$ a good approximation to $f'(x)$ within 0.0001 units.

\subsection{Gradient Checking in Practice}

Reshape $W^{[1]}$, $b^{[1]}$, ... $W^{[L]}$, $b^{[L]}$ and $dW^{[1]}$, $db^{[1]}$, ... $dW^{[L]}$, $db^{[L]}$ into large, consolidated vectors, $\theta$ and $d\theta$, respectively.

Now, our cost function is $J(\theta)$ instead of $J(W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]})$.  Therefore, we can write $J(\theta)$ as

\begin{align}
J(\theta) = J(\theta_1, \theta_2, ..., \theta_L)
\end{align}

For each dimension (or layer) of the function, we must compute $d\theta_{approx}$ as in Algorithm \ref{alg:grad_check}:

\begin{algorithm}[h]
\caption{Gradient Check}
\label{alg:grad_check}
\For{i:=1 to L}{
$d\theta_{approx}[i] := \frac{J(\theta_1, \theta_2, ..., \theta_i + \epsilon, ..., \theta_L) + J(\theta_1, \theta_2, ..., \theta_i - \epsilon, ..., \theta_L)}{2 \epsilon}$\;
}
\end{algorithm}

If $\theta$ has been chosen wisely, then

\begin{align}
d\theta_{approx}[i] \approx \frac{\partial{J}}{\partial{\theta_i}}
\end{align}

To compute the above formula, you can use the normalized Frobenius distance (e.g. Euclidean distance) referenced from Equation \ref{eqn:frobenius}:

\begin{align}
e = \frac{||d\theta_{approx} - d\theta||_2}{||d\theta_{approx}||_2 + ||d\theta||_2||}
\end{align}

In general, if you choose any value $\epsilon$ and the ratio above yields $e \leq \epsilon$, then your approximation is most likely good.  If your approximation is off by two orders of magnitude, it may be alright, but you will need to check all your components.  If it's off by higher orders of magnitude, it is most likely incorrect.

\subsubsection{Implementation Notes}

\begin{itemize}
\item Don't use gradient checking in training, only to debug, because it is very slow.
\item If grad check fails, look at each component of $d\theta_{approx}$ to identify which layer is causing problems.  You should notice that the component is quite a bit different than the similar component in $d\theta$.
\item Grad check doesn't work with dropout regularization, because dropout is randomized and makes it difficult to compute $J$.
\item Perform random initialization and train the model for a while before running grad check to let $W$ and $b$ wander away from the $\approx 0$ starting values.
\end{itemize}

\end{document}