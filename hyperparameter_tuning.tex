\documentclass{article}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}

\newcommand\todo[1]{\textcolor{red}{TODO #1}}

\begin{document}

\title {Hyperparameter Tuning}
\author{William Grim \\ \href{mailto:grimwm@gmail.com}{grimwm@gmail.com}}

\maketitle

\tableofcontents

\section{Hyperparameter Tuning}

\paragraph{Hyperparameter Sampling}

To determine which hyperparameters are the most important to tune, randomly sample hyperparameters and train with them.  This will help determine which ones are most important.

Having said this, generally speaking, $\alpha$ is the most important parameter to tune.  Afterwards, it is \textit{\# hidden units}, \textit{mini-batch size}, etc.  This choice of which ones are important will come with experience, but, nevertheless, use \textit{hyperparameter sampling} to find the appropriate values.

\paragraph{Random Number Scale}

Do not use linear random sampling, because with denominators that are very sensitive to values close to 1 (e.g. $\frac{1}{1-\beta}$), normal linear random sampling can dramatically change results.  Instead, use log-scale random sampling to choose "r" (e.g. choose \textit{r=-k*np.random.rand()} for a value of "r" between -k and 0).  Then, the actual random value becomes $10^r$, or in the case of certain formulas, it might be chosen as $1 - 10^r$.

\paragraph{Re-Test}

As hardware, data, and software change, the choice of hyperparameters can become stale.  Therefore, it's important to retest the optimal values of the hyperparameters at least once every few months.

\paragraph{Babysitting One Model vs. Training Many Models in Parallel}

In the babysitting one model approach, one training model is used over the course of many hours or days, and the learning rate is nudged every so often to try and optimize the costs.  This approach is generally used when there are not enough computing resources for training in parallel, which can happen with massive amounts of data.

In the parallel training scenario, many training models are run in parallel.  After some time, graphs of the costs can be compared so the best model can be chosen.  This is generally acceptable when there are lots of computer resources for training.

\end{document}